{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ok7ywfd3qfbj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as dataloader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformation_operation= transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "Thh6S8DZrHPZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset= torchvision.datasets.MNIST(root='./data',\n",
        "                                          train= True,\n",
        "                                          download= True,\n",
        "                                          transform=transformation_operation )\n",
        "\n",
        "val_dataset= torchvision.datasets.MNIST(root='./data',\n",
        "                                          train= False,\n",
        "                                          download= True,\n",
        "                                          transform=transformation_operation )"
      ],
      "metadata": {
        "id": "htZvDYylqzfU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "num_classes=10\n",
        "num_channels=1 #since black and white image\n",
        "img_size=28\n",
        "patch_size=7\n",
        "patch_num= (img_size//patch_size)*(img_size//patch_size)\n",
        "attention_heads=4\n",
        "embed_dim =  20    #size of the embedding vector\n",
        "transformer_blocks=4\n",
        "mlp_nodes= 64        #number of neurons in the mlp hidden layers\n",
        "learning_rate= 0.001"
      ],
      "metadata": {
        "id": "Z0WkzFsTr75I"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to batches\n",
        "train_data= dataloader.DataLoader(train_dataset, shuffle= True, batch_size=batch_size)\n",
        "val_data= dataloader.DataLoader(val_dataset, shuffle= True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Pebn9BDXq0sl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class for patch embedding- part 1 of VIT\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self):                 #we'll not code attention from scratch- its not our goal\n",
        "    super().__init__()\n",
        "    self.patch_embed= nn.Conv2d(num_channels, embed_dim, kernel_size= patch_size, stride=patch_size)\n",
        "    # initial size of image(61,16,embed_dim)-> (batch_size, embed_dim, num_patches_per_row, num_patches_per_col)\n",
        "  def forward(self,x):\n",
        "    x= self.patch_embed(x)\n",
        "    x= x.flatten(2)\n",
        "    x=x.transpose(1,2)\n",
        "    return x\n",
        "    # or return self.patch_embed(x).flatten(2).transpose(1,2)\n",
        ""
      ],
      "metadata": {
        "id": "oVaqG3w1q0vD"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_data))\n",
        "print(images.shape)\n",
        "\n",
        "patch_embed= nn.Conv2d(num_channels, embed_dim, kernel_size= patch_size, stride=patch_size)\n",
        "embedded_image= patch_embed(images)\n",
        "print(\"this is the size after conv2d \",embedded_image.shape)\n",
        "embedded_image[:5]\n",
        "# print(\"hare krishna \",embedded_image.flatten(2))\n",
        "print(\"hare krishna \",embedded_image.flatten(2).transpose(1,2).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDjsPOatyyZ5",
        "outputId": "c59cc5c5-0023-4be1-ce0b-3eb88b572bf1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "this is the size after conv2d  torch.Size([64, 20, 4, 4])\n",
            "hare krishna  torch.Size([64, 16, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wrOTByO_yyQo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fugn9e53yyNL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class for transformer encoder- Part 2 to VIt\n",
        "#Layer normalisation\n",
        "#multihead attention\n",
        "# layer normalisation\n",
        "# residuals\n",
        "# mlp and activation function\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_norm1= nn.LayerNorm(embed_dim)\n",
        "    self.multi_head_attention= nn.MultiheadAttention(embed_dim,attention_heads, batch_first=True)\n",
        "    self.layer_norm2= nn.LayerNorm(embed_dim)\n",
        "    self.mlp= nn.Sequential(\n",
        "          nn.Linear(embed_dim, mlp_nodes),\n",
        "          nn.GELU(),\n",
        "          nn.Linear(mlp_nodes, embed_dim)\n",
        "          # nn.GELU(),\n",
        "          # nn.Linear(embed_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual1=x\n",
        "    x= self.layer_norm1(x)\n",
        "    x= self.multi_head_attention(x,x,x)[0]  +residual1 #for k, q, v\n",
        "    residual2= x\n",
        "    x= self.layer_norm2(x)\n",
        "    x= self.mlp(x) + residual2\n",
        "    return x\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "aOHmS7yzq0xi"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class for MLP head for classification - PART 3 of VIT\n",
        "class MLP_Head(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layernorm1= nn.LayerNorm(embed_dim)   #this is not shown in paper, but its done\n",
        "    self.mlphead= nn.Sequential(\n",
        "        # nn.Linear(embed_dim)\n",
        "        nn.Linear(embed_dim,num_classes)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    # x=x[:,0]   #cls token\n",
        "    x= self.layernorm1(x)\n",
        "    x= self.mlphead(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1OSbYGqMq00I"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patch_embedding= PatchEmbedding()\n",
        "    self.cls_token=nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "    self.position_embedding=nn.Parameter(torch.randn(1,patch_num +1, embed_dim))\n",
        "    self.transformer_blocks= nn.Sequential(* [TransformerEncoder() for _ in range (transformer_blocks) ])\n",
        "    self.mlp_head=MLP_Head()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch_embedding(x)\n",
        "    B=x.shape[0]   #B= batch_size is not perfect, what if total images in last batch not divisible by batch size\n",
        "    cls_tokens= self.cls_token.expand(B,-1,-1)\n",
        "    x=torch.cat((cls_tokens,x),1)\n",
        "    x=x+self.position_embedding\n",
        "    x= self.transformer_blocks(x)\n",
        "    x=x[:,0]   #taking only the cls\n",
        "    x= self.mlp_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "edpK4pFRq02t"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimiser, device, crossentopyloss\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model= VisionTransformer().to(device)\n",
        "optimizer= torch.optim.Adam(VisionTransformer().parameters(), lr= learning_rate)\n",
        "criterion= nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lUbZriw8FYii"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model, optimizer and loss function setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VisionTransformer().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for images, labels in train_data:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)              # Forward pass\n",
        "        loss = criterion(outputs, labels)    # Loss calculation\n",
        "        loss.backward()                      # Backpropagation\n",
        "        optimizer.step()                     # Optimizer step\n",
        "\n",
        "        predicted = outputs.argmax(dim=1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    train_acc = 100 * total_correct / total_samples\n",
        "    print(f\"Epoch {epoch+1}: Training Accuracy {train_acc:.2f}%\")\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_data:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_samples += labels.size(0)\n",
        "    val_acc = 100 * val_correct / val_samples\n",
        "    print(f\"Epoch {epoch+1}: Validation Accuracy {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Vf2B9qpHq05h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225b186b-b024-4e1f-a291-d7f1b5be29d0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Accuracy 82.40%\n",
            "Epoch 1: Validation Accuracy 93.38%\n",
            "Epoch 2: Training Accuracy 94.16%\n",
            "Epoch 2: Validation Accuracy 95.02%\n",
            "Epoch 3: Training Accuracy 95.58%\n",
            "Epoch 3: Validation Accuracy 95.55%\n",
            "Epoch 4: Training Accuracy 96.30%\n",
            "Epoch 4: Validation Accuracy 96.34%\n",
            "Epoch 5: Training Accuracy 96.87%\n",
            "Epoch 5: Validation Accuracy 96.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05TWN2u5IMJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYcoT7wgq08d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hIz-RxxPq0_Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vtg40Rw1q1DB"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}